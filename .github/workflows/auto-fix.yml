name: Auto-Fix Test Failures

on:
  issues:
    types: [opened, labeled]
  workflow_dispatch:
    inputs:
      issue_number:
        description: 'Issue number to fix (optional - will run tests if not provided)'
        required: false
        type: number
      difficulty:
        description: 'Test difficulty level'
        required: false
        default: 'easy'
        type: choice
        options:
          - easy
          - medium
          - hard
          - all
          - sample-fail
      ai_model:
        description: 'AI model to use'
        required: false
        default: 'gpt-4o-mini'
        type: choice
        options:
          - gpt-4o-mini
          - gpt-4o
          - claude-3-5-sonnet-20240620
          - deepseek/deepseek-chat
          - deepseek/deepseek-coder

jobs:
  attempt-auto-fix:
    name: Attempt Auto-Fix
    # Only run if issue has 'auto-fix-ready' label OR manual trigger
    if: |
      github.event_name == 'workflow_dispatch' ||
      contains(github.event.issue.labels.*.name, 'auto-fix-ready')
    runs-on: ubuntu-latest

    permissions:
      contents: write      # To create branches and commits
      issues: write        # To comment on issues
      pull-requests: write # To create PRs

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Get workflow parameters
        id: params
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "issue_number=${{ inputs.issue_number }}" >> $GITHUB_OUTPUT
            echo "ai_model=${{ inputs.ai_model }}" >> $GITHUB_OUTPUT
            echo "difficulty=${{ inputs.difficulty }}" >> $GITHUB_OUTPUT
          else
            echo "issue_number=${{ github.event.issue.number }}" >> $GITHUB_OUTPUT
            echo "ai_model=gpt-4o-mini" >> $GITHUB_OUTPUT
            echo "difficulty=sample-fail" >> $GITHUB_OUTPUT
          fi

          # Set confidence threshold based on difficulty
          case "${{ inputs.difficulty }}" in
            easy)
              echo "min_confidence=0.90" >> $GITHUB_OUTPUT
              ;;
            medium)
              echo "min_confidence=0.75" >> $GITHUB_OUTPUT
              ;;
            hard)
              echo "min_confidence=0.60" >> $GITHUB_OUTPUT
              ;;
            all)
              echo "min_confidence=0.75" >> $GITHUB_OUTPUT
              ;;
            *)
              echo "min_confidence=0.75" >> $GITHUB_OUTPUT
              ;;
          esac

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps chromium

      - name: Run tests to generate failures
        id: tests
        run: |
          set +e  # Don't exit on test failure

          DIFFICULTY="${{ steps.params.outputs.difficulty }}"
          echo "üß™ Running tests for difficulty: $DIFFICULTY"

          case "$DIFFICULTY" in
            easy)
              npm run test:easy
              ;;
            medium)
              npm run test:medium
              ;;
            hard)
              npm run test:hard
              ;;
            all)
              npm run test:all-difficulties
              ;;
            sample-fail)
              npm run test:fail
              ;;
            *)
              npm test
              ;;
          esac

          TEST_EXIT_CODE=$?
          echo "exit-code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT
          exit 0  # Always succeed this step
        continue-on-error: true

      - name: Check if failures exist
        id: check_failures
        run: |
          if [ ! -f "playwright-report/results.json" ]; then
            echo "‚ùå No test results found"
            echo "failures_found=false" >> $GITHUB_OUTPUT
            exit 1
          fi

          # Check if there are actual failures in the results
          FAILURE_COUNT=$(jq '[.suites[].specs[].tests[] | select(.status == "failed" or .status == "timedOut")] | length' playwright-report/results.json)

          if [ "$FAILURE_COUNT" -eq 0 ]; then
            echo "‚úÖ No test failures found - nothing to fix!"
            echo "failures_found=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "üìä Found $FAILURE_COUNT test failures"
          echo "failures_found=true" >> $GITHUB_OUTPUT
          echo "failure_count=$FAILURE_COUNT" >> $GITHUB_OUTPUT

      - name: Skip if no failures
        if: steps.check_failures.outputs.failures_found != 'true'
        run: |
          echo "No failures to fix - exiting gracefully"

          # Only comment on issue if issue number was provided
          if [ -n "${{ steps.params.outputs.issue_number }}" ]; then
            gh issue comment ${{ steps.params.outputs.issue_number }} --body "## ‚ÑπÔ∏è No Failures Found

            Re-ran tests but found no failures to fix. The tests may have been fixed already or the issue may be resolved.

            ü§ñ Auto-Fix Workflow"
          fi
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Dagger
        uses: dagger/dagger-for-github@v6
        with:
          version: "latest"
          verb: ""  # Don't run dagger automatically, we'll run it in the next step

      - name: Run Dagger auto-fix
        if: steps.check_failures.outputs.failures_found == 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          # AI API keys - at least one must be set
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: |
          echo "üöÄ Starting Dagger auto-fix..."
          echo "   Difficulty: ${{ steps.params.outputs.difficulty }}"
          echo "   Model: ${{ steps.params.outputs.ai_model }}"
          echo "   Failures: ${{ steps.check_failures.outputs.failure_count }}"
          echo "   Min Confidence: ${{ steps.params.outputs.min_confidence }}"

          # Check if issue number was provided, otherwise create a tracking issue
          ISSUE_NUMBER="${{ steps.params.outputs.issue_number }}"

          if [ -z "$ISSUE_NUMBER" ]; then
            echo "   No issue number provided - creating tracking issue..."

            ISSUE_NUMBER=$(gh issue create \
              --title "Auto-fix: ${{ steps.params.outputs.difficulty }} difficulty with ${{ steps.params.outputs.ai_model }}" \
              --label "automated-fix,playground" \
              --body "Auto-Fix Experiment - Difficulty: ${{ steps.params.outputs.difficulty }}, Model: ${{ steps.params.outputs.ai_model }}, Min Confidence: ${{ steps.params.outputs.min_confidence }}, Failures: ${{ steps.check_failures.outputs.failure_count }}. [View run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" \
              --json number --jq '.number')

            echo "   Created issue: #$ISSUE_NUMBER"
          fi

          echo "   Issue: #$ISSUE_NUMBER"
          echo "   Creating PR if fixes are found..."

          dagger -m dagger call fix-and-create-pr \
            --repo-dir=. \
            --failures-json-path=playwright-report/results.json \
            --issue-number=$ISSUE_NUMBER \
            --github-token=env:GITHUB_TOKEN \
            --ai-model=${{ steps.params.outputs.ai_model }} \
            --min-confidence=${{ steps.params.outputs.min_confidence }} \
            --repository=${{ github.repository }} \
            --base-branch=main \
            --openai-api-key=env:OPENAI_API_KEY \
            --anthropic-api-key=env:ANTHROPIC_API_KEY \
            --deepseek-api-key=env:DEEPSEEK_API_KEY

      - name: Upload test results artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ steps.issue.outputs.number }}
          path: |
            playwright-report/
            test-results/
          retention-days: 7

      - name: Comment on issue if failed
        if: failure() && steps.params.outputs.issue_number != ''
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh issue comment ${{ steps.params.outputs.issue_number }} --body "## ‚ùå Auto-Fix Failed

          The auto-fix workflow encountered an error and could not complete.

          **Configuration:**
          - Difficulty: ${{ steps.params.outputs.difficulty }}
          - Model: ${{ steps.params.outputs.ai_model }}
          - Min Confidence: ${{ steps.params.outputs.min_confidence }}

          **Troubleshooting:**
          - Verify that AI API keys are configured in repository secrets
          - Check that tests generated valid failures
          - Review the workflow logs for detailed error messages

          [View workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          ü§ñ Auto-Fix Workflow"
